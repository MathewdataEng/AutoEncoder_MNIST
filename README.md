# AutoEncoder_MNIST
AutoEncoders (AE) are a captivating subset of neural networks known for their prowess in data compression and feature extraction. Their operation involves two key steps:
1. Encoding: Input data is transformed into a concise representation, often referred to as the Latent Code.
2. Decoding: This latent code is then used to reconstruct the original input.

This process holds immense value across a spectrum of applications, from image and speech recognition to anomaly detection and more. Autoencoders possess the ability to unveil concealed patterns and reduce data dimensionality, rendering them an indispensable tool for data scientists and machine learning enthusiasts.

For instance, consider the MNIST dataset comprising 28x28 pixel images. Traditionally, these images demand approximately 3136 bytes for storage, but with the magic of Autoencoders, a mere 128 bytes can capture the essence of each image through the compact latent code.

## Result Image
![image](https://github.com/MathewdataEng/AutoEncoder_MNIST/assets/114323097/33bbbad8-3046-4485-9d7f-82c017583bdc)

